{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 250.7MiB...\n",
      "Wrote labels to file.\n",
      "Processing chunk 0, size=28.0MiB\n",
      "(5784, 54, 54) (5784,)\n",
      "Working with 5784 (1000 unique) sounds, including silence. )\n",
      "Processing chunk 1, size=27.7MiB\n",
      "(6296, 54, 54) (6296,)\n",
      "Working with 6296 (1000 unique) sounds, including silence. )\n",
      "Processing chunk 2, size=27.5MiB\n",
      "(6140, 54, 54) (6140,)\n",
      "Working with 6140 (1000 unique) sounds, including silence. )\n",
      "Processing chunk 3, size=27.4MiB\n",
      "(6022, 54, 54) (6022,)\n",
      "Working with 6022 (1000 unique) sounds, including silence. )\n",
      "Processing chunk 4, size=27.7MiB\n",
      "(6074, 54, 54) (6074,)\n",
      "Working with 6074 (1000 unique) sounds, including silence. )\n",
      "Processing chunk 5, size=27.4MiB\n",
      "(6120, 54, 54) (6120,)\n",
      "Working with 6120 (1000 unique) sounds, including silence. )\n",
      "Processing chunk 6, size=27.6MiB\n",
      "(5915, 54, 54) (5915,)\n",
      "Working with 5915 (1000 unique) sounds, including silence. )\n",
      "Processing chunk 7, size=27.5MiB\n",
      "(6022, 54, 54) (6022,)\n",
      "Working with 6022 (1000 unique) sounds, including silence. )\n",
      "Processing chunk 8, size=27.5MiB\n",
      "(6080, 54, 54) (6080,)\n",
      "Working with 6080 (1000 unique) sounds, including silence. )\n",
      "Processing chunk 9, size=2.4MiB\n",
      "(480, 54, 54) (480,)\n",
      "Working with 480 (90 unique) sounds, including silence. )\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import glob\n",
    "import os.path as path\n",
    "import pickle\n",
    "import re\n",
    "import gc\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from listen.spectrogram.spectrogram import Spectrogram\n",
    "from listen.utils.filters import Filter\n",
    "\n",
    "\"\"\"\n",
    "Utility functions\n",
    "\"\"\"\n",
    "\n",
    "def db_to_float(db):\n",
    "    return 10 ** (db / 20)\n",
    "\n",
    "def chunk_it(xs, chunk_size):\n",
    "    for i in range(0, len(xs), chunk_size):\n",
    "        yield xs[i:i + chunk_size]\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"{:3.1f}{}{}\".format(num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"{:.1f}{}{}\".format(num, 'Yi', suffix)\n",
    "\n",
    "\"\"\"\n",
    "Constants.\n",
    "\"\"\"\n",
    "\n",
    "DATA_DIR = path.realpath('./listen/data/gen/proc')\n",
    "DATASET_PATH = './listen/data/gen/pickled/'\n",
    "DATASET_PATH_FMT = DATASET_PATH + 'data_batch_{}.pkl'\n",
    "MAX_NB_COPIES = 12\n",
    "\n",
    "# 1% of the dataset is silence\n",
    "SILENCE_RELATIVE_COUNT = 0.01\n",
    "SILENCE_RANGE_DB = (-60, -40)\n",
    "\n",
    "# Sample rate\n",
    "RATE = 22050\n",
    "\n",
    "# Speech duration in seconds for pitch-invariant scaling\n",
    "SPEECH_DURATION = 0.7\n",
    "\n",
    "\n",
    "FFT_SIZE = 2048\n",
    "STEP_SIZE = FFT_SIZE // 8\n",
    "LOG_THRESHOLD = 6\n",
    "CHUNK_SIZE = 1000\n",
    "NB_MFCC_BINS = 54\n",
    "FREQ_RANGE = (0, 8000)\n",
    "\"\"\"\n",
    "Generating the dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Get all the wav files\n",
    "wavs = list(glob.iglob(path.join(DATA_DIR, '**/*.wav'), recursive=True))\n",
    "\n",
    "# Print total size being read\n",
    "print(\"Reading: {}...\".format(sizeof_fmt(sum(path.getsize(w) for w in wavs))))\n",
    "\n",
    "utterances = list(set(map(lambda w: re.match(r'[a-z]+', path.basename(w)).group(0), wavs)))\n",
    "\n",
    "# Dump the labels\n",
    "data_file = open(path.realpath(DATASET_PATH + 'all_labels.pkl'), \"wb\")\n",
    "pickle.dump({'label_names': utterances}, data_file, pickle.HIGHEST_PROTOCOL)\n",
    "data_file.close()\n",
    "print('Wrote labels to file.')\n",
    "\n",
    "glob_max = -1\n",
    "\n",
    "# Shuffle input files\n",
    "np.random.shuffle(wavs)\n",
    "\n",
    "spec = Spectrogram(FFT_SIZE, STEP_SIZE, LOG_THRESHOLD)\n",
    "\n",
    "for chunk_idx, chunk in enumerate(chunk_it(wavs, CHUNK_SIZE)):\n",
    "    print(\"Processing chunk {}, size={}\".format(chunk_idx, sizeof_fmt(sum(path.getsize(c) for c in chunk))))\n",
    "    cepstra = []\n",
    "    utterance_idx = []\n",
    "    dump_data = {}\n",
    "    \n",
    "    for wav in chunk:\n",
    "        rate, snd = wavfile.read(wav, mmap=True)\n",
    "        m = np.max(snd)\n",
    "        glob_max = max(m, glob_max)\n",
    "        # Split file name to extract utterance\n",
    "        utterance = re.match(r'[a-z]+', path.basename(wav)).group(0)\n",
    "        uidx = utterances.index(utterance)\n",
    "        # Poor man's data variance\n",
    "        noise_db = np.random.uniform(*SILENCE_RANGE_DB)\n",
    "        noise_level = m * db_to_float(noise_db)\n",
    "        nb_copies = np.random.randint(1, MAX_NB_COPIES)\n",
    "        for i in range(nb_copies):\n",
    "            noise = np.random.normal(0, noise_level, len(snd))\n",
    "            factor = SPEECH_DURATION * rate / len(snd)\n",
    "            sound = Filter.time_stretch(snd + noise, factor)\n",
    "            sound = sound / np.max(sound)\n",
    "            cep = spec.compute_mel_cepstrum(sound.astype('float32'), nb_mfcc_bins=NB_MFCC_BINS, frange=FREQ_RANGE)\n",
    "            # Ensure square shape with piecewise linear interpolation\n",
    "            if cep.shape[0] != cep.shape[1]:\n",
    "                interpolant = interp1d(np.linspace(0, 1, cep.shape[1]), cep, axis=1, fill_value='extrapolate')\n",
    "                cep = interpolant(np.linspace(0, 1, cep.shape[0]))\n",
    "            cepstra.append(cep - np.mean(cep))\n",
    "            # Reserve index 0 for use later\n",
    "            utterance_idx.append(uidx + 1)        \n",
    "    \"\"\"\n",
    "    Add silence as data\n",
    "    \"\"\"\n",
    "    # Manually add noise as silence now\n",
    "    nb_silent = int(SILENCE_RELATIVE_COUNT * len(cepstra))\n",
    "    for i in range(nb_silent):\n",
    "        noise_db = np.random.uniform(*SILENCE_RANGE_DB)\n",
    "        noise_level = (10 ** (noise_db / 20)) * glob_max\n",
    "        noise = np.random.normal(0, noise_level, len(snd))\n",
    "        cep = spec.compute_mel_cepstrum(noise.astype('float32'), nb_mfcc_bins=NB_MFCC_BINS, frange=FREQ_RANGE)\n",
    "        if cep.shape[0] != cep.shape[1]:\n",
    "            interpolant = interp1d(np.linspace(0, 1, cep.shape[1]), cep, axis=1, fill_value='extrapolate')\n",
    "            cep = interpolant(np.linspace(0, 1, cep.shape[0]))\n",
    "        cepstra.append(cep)\n",
    "        # Silence is index 0\n",
    "        utterance_idx.append(0)\n",
    "\n",
    "    # Shuffle again to avoid keeping copies together\n",
    "    nb_data = len(cepstra)\n",
    "    perm = np.random.permutation(nb_data)\n",
    "    cepstra = np.array(cepstra)\n",
    "    utterance_idx = np.array(utterance_idx)\n",
    "    # Shuffle axis 0\n",
    "    cepstra = cepstra[perm, :, :]\n",
    "    utterance_idx = utterance_idx[perm]\n",
    "    \n",
    "    print(cepstra.shape, utterance_idx.shape)\n",
    "    \n",
    "    # Just a sanity check assertion\n",
    "    assert(cepstra.shape[0] == utterance_idx.shape[0]), \"Data, Label vectors shapes do not match!\"\n",
    "\n",
    "    print(\"Working with {0} ({1} unique) sounds, including silence. )\".format(nb_data, len(chunk)))\n",
    "\n",
    "    \"\"\"\n",
    "    Serializing the dataset.\n",
    "    \"\"\"\n",
    "    # Split into training and validation sets (10% for validation, 90% for training)\n",
    "    dump_data = {\n",
    "        'validation_x': cepstra[nb_data//10:],\n",
    "        'validation_y': utterance_idx[nb_data//10:], \n",
    "        'training_x': cepstra[:nb_data//10],\n",
    "        'training_y': utterance_idx[:nb_data//10]\n",
    "    }\n",
    "    \n",
    "    # Dump dataset\n",
    "    data_file = open(DATASET_PATH_FMT.format(chunk_idx), \"wb\")\n",
    "    pickle.dump(dump_data, data_file, pickle.HIGHEST_PROTOCOL)\n",
    "    data_file.close()\n",
    "    # Trigger GC to clear all the massive number of references we now have remaining \n",
    "    gc.collect()\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from listen.utils.filters import Filter\n",
    "\n",
    "FFT_SIZE = 2048\n",
    "STEP_SIZE = FFT_SIZE // 8\n",
    "LOG_THRESHOLD = 6\n",
    "\n",
    "spec = Spectrogram(FFT_SIZE, STEP_SIZE, LOG_THRESHOLD)\n",
    "name = \"D:\\\\theyenaman\\\\Dropbox\\\\code\\\\python\\\\ml_project\\\\listen\\\\data\\\\gen\\\\proc\\\\alex-default\\\\a.wav\"\n",
    "rate, data = wavfile.read(name)\n",
    "factor = 0.7 * rate / len(data)\n",
    "# data =  data.astype(np.float32) + np.random.normal(0, (1e-2) * np.max(data), len(data))\n",
    "data_stretched = Filter.time_stretch(data, factor)\n",
    "data_stretched = data_stretched / np.max(data_stretched)\n",
    "mel_cepstrum = spec.compute_mel_cepstrum(data_stretched.astype('float32'), nb_mfcc_bins=54, frange=(0, 8000))\n",
    "print(mel_cepstrum.shape)\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(10,10))\n",
    "if(mel_cepstrum.shape[0] != mel_cepstrum.shape[1]):\n",
    "mel_cepstrum = (interp1d(np.linspace(0, 1, mel_cepstrum.shape[1]), mel_cepstrum, axis=1))(np.linspace(0, 1, mel_cepstrum.shape[0]))\n",
    "print(mel_cepstrum.shape)\n",
    "ax.matshow(mel_cepstrum, interpolation='nearest', aspect='auto', cmap=plt.cm.afmhot, origin='lower')\n",
    "plt.show()\n",
    "IPython.display.Audio(data=(data_stretched), rate=rate)\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
